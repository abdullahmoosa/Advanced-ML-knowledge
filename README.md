# Advanced-ML-knowledge
Collection of resources which I found interesting while learning ML

1. “Grounding Dino” is a zero shot learning model. Something like Image transformers. It does not require labelling while training.  Link :  https://blog.roboflow.com/grounding-dino-zero-shot-object-detection/
2. Autoencoders can be used for image noise filtering and anomaly detection from data. Link : https://blog.roboflow.com/grounding-dino-zero-shot-object-detection/
3. For Anomaly Detection :  https://www.youtube.com/watch?v=6S2v7G-OupA
4. For tracking object and counting unique object Deep SORT is used. Link:  https://github.com/abewley/sort
5. BERT and GPT. :  https://www.youtube.com/watch?v=9Y7f4j396hI
6. GPT is mainly focused on “Decoding” part.
7. BERT is focused on “Encoding” part.
8. Important steps regarding tflite :  https://www.tensorflow.org/lite
9. Take a look into YOLO-NAS :  https://blog.roboflow.com/yolo-nas-how-to-train-on-custom-dataset/
10. Snapshot Ensemble Deep Learning :  https://machinelearningmastery.com/snapshot-ensemble-deep-learning-neural-network/
11. Selective search and other search techniques in deep learning :  https://learnopencv.com/selective-search-for-object-detection-cpp-python/#:~:text=Selective%20Search%20is%20a%20region,texture%2C%20size%20and%20shape%20compatibility.
12. High precision vs high recall :  https://chat.openai.com/share/4d2aabd1-ab0a-4532-8eda-f727d755c22f
13. Working principle and comparison of object detection algorithms :  https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e
14. Graph convolutional network : https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b
15. Data drift and concept drift in machine learning : https://towardsdatascience.com/machine-learning-in-production-why-you-should-care-about-data-and-concept-drift-d96d0bc907fb
16. Internal Covariate Shift in Machine Learning : https://medium.com/analytics-vidhya/internal-covariate-shift-an-overview-of-how-to-speed-up-neural-network-training-3e2a3dcdd5cc
17. For Error Analysis in ML model:  https://erroranalysis.ai/,  https://fairlearn.org/
18. Experiment Tracking :  https://neptune.ai/blog/ml-experiment-tracking
19. Best practices with Tensorflow :  https://cs230.stanford.edu/blog/datapipeline/#best-practices
20. Feedback loop for Active Learning labelling :  https://www.linkedin.com/pulse/feedback-loop-machine-learning-labeling-data-sanchit-tiwari
21. Analyzing log files framework :  https://www.elastic.co/logstash
22. Know more about Elastic Search, seems interesting : https://www.knowi.com/blog/what-is-elastic-search/
23. How Ber tokenizer works : https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/
24. "Concept Drift VS Data Drift in Machine Learning" : https://nulltoinfinity.hashnode.dev/concept-drift-and-data-drift-in-machine-learning
25. It is important to understand data distribution before using the dataset. And also find out if the dataset is good/fair. Facet is an interesting library/ software. [https://pair-code.github.io/facets/](url)
26. A feature column should not be included in prediction if it is not significant enough. Assume a binary classification model, If the data distribution of a feature for class 0 is similar to the data distribution for class 1 then the feature column might not bear much significance.
27. Data collection, processing guideline : [https://pair.withgoogle.com/chapter/data-collection/](url)
28. To inspect model after training : [https://pair-code.github.io/what-if-tool/](url)
29. To understand NLP models after training : [https://pair-code.github.io/lit/](url)
